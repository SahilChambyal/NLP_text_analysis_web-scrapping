{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XgiUodLwj9r",
        "outputId": "66d8abc4-8d79-441e-a4d1-2008cfa0bb92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.11/dist-packages (0.2.8)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.11/dist-packages (0.4.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.11/dist-packages (0.7.4)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (11.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.3.0)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.32.3)\n",
            "Requirement already satisfied: feedparser>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (6.0.11)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.11/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.11/dist-packages (from textstat) (0.17.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from textstat) (75.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.11/dist-packages (from feedparser>=5.2.1->newspaper3k) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.10.0->newspaper3k) (2024.12.14)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.17.0)\n"
          ]
        }
      ],
      "source": [
        "# All the necessary dependencies for this program\n",
        "!pip install newspaper3k lxml_html_clean nltk textstat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPJd4wXseMse",
        "outputId": "03652b63-dbab-4b39-b99c-6a27db5eaf1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# all the necessary modules\n",
        "from newspaper import Article\n",
        "import nltk\n",
        "import re\n",
        "import textstat\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6jXnAMtyNKI5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# All the necessary sub-functions\n",
        "\n",
        "def load_words(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"latin-1\") as file:\n",
        "        return set(file.read().split())\n",
        "def sentiment_analysis(text):\n",
        "    words = word_tokenize(text.lower())\n",
        "    words = [word for word in words if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    positive_score = sum(1 for word in words if word in positive_words)\n",
        "    negative_score = sum(1 for word in words if word in negative_words)\n",
        "\n",
        "    negative_score = abs(negative_score)\n",
        "\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
        "\n",
        "    return {\n",
        "        \"POSITIVE SCORE\": positive_score,\n",
        "        \"NEGATIVE SCORE\": negative_score,\n",
        "        \"POLARITY Score\": round(polarity_score, 4),\n",
        "        \"SUBJECTIVITY Score\": round(subjectivity_score, 4),\n",
        "    }\n",
        "\n",
        "def readability_analysis(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    words_cleaned = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "    avg_sentence_length = sum(len(sentence) for sentence in sentences) / len(sentences) if sentences else 0\n",
        "    complex_words = [word for word in words_cleaned if textstat.syllable_count(word) > 2]\n",
        "    complex_word_percentage = len(complex_words) / len(words_cleaned) if words_cleaned else 0\n",
        "\n",
        "    fog_index = 0.4 * (avg_sentence_length + complex_word_percentage)\n",
        "\n",
        "    return {\n",
        "        \"AVG SENTENCE LENGTH\": round(avg_sentence_length, 2),\n",
        "        \"PERCENTAGE OF COMPLEX WORDS\": round(complex_word_percentage * 100, 2),\n",
        "        \"FOG INDEX\": round(fog_index, 2),\n",
        "        \"AVG NUMBER OF WORDS PER SENTENCE\": round(len(words_cleaned) / len(sentences) if sentences else 0, 2),\n",
        "        \"COMPLEX WORD COUNT\": len(complex_words),\n",
        "    }\n",
        "\n",
        "\n",
        "def word_analysis(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = word_tokenize(text)\n",
        "    words_cleaned = [word for word in words if word.isalnum() and word.lower() not in stop_words]\n",
        "\n",
        "    total_words = len(words_cleaned)\n",
        "    total_chars = sum(len(word) for word in words_cleaned)\n",
        "    avg_word_length = total_chars / total_words if total_words else 0\n",
        "    avg_words_per_sentence = total_words / len(sentences) if sentences else 0\n",
        "\n",
        "    return {\n",
        "        \"WORD COUNT\": total_words,\n",
        "        \"AVG WORD LENGTH\": round(avg_word_length, 2),\n",
        "    }\n",
        "def personal_pronouns(text):\n",
        "    pronouns = re.findall(r\"\\b(I|we|my|ours|us)\\b\", text, re.I)\n",
        "    return {\"PERSONAL PRONOUNS\": len(pronouns)}\n",
        "\n",
        "def cleanText(text):\n",
        "    lines = text.split(\"\\n\")\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.strip().lower().startswith(\"summarize\"):\n",
        "            return \"\\n\".join(lines[:i])\n",
        "    return text\n",
        "\n",
        "def count_syllables(word):\n",
        "    word = word.lower()\n",
        "    vowels = \"aeiou\"\n",
        "\n",
        "    syllable_count = len(re.findall(r'[aeiouy]+', word))\n",
        "\n",
        "    if word.endswith((\"es\", \"ed\")) and len(word) > 2:\n",
        "        syllable_count -= 1\n",
        "\n",
        "    return max(1, syllable_count)\n",
        "\n",
        "\n",
        "def calculate_syllable_per_word(text):\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    total_syllables = sum(count_syllables(word) for word in words)\n",
        "    total_words = len(words)\n",
        "    res =  total_syllables / total_words if total_words > 0 else 0\n",
        "    return {\"SYLLABLE PER WORD\": res }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vLgPCquNb-HB"
      },
      "outputs": [],
      "source": [
        "# Loading URLs and helper files\n",
        "\n",
        "positive_words = load_words(\"/content/positive-words.txt\") #File address for positive words\n",
        "negative_words = load_words(\"/content/negative-words.txt\") #File address for negative words\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"/content/Output Data Structure.xlsx\") #File address for input file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Z0P0y6VRb7mt"
      },
      "outputs": [],
      "source": [
        "# Loop over all the URLs and storing data in df\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        clean_text = article.title + cleanText(article.text)\n",
        "\n",
        "        sentiment_results = sentiment_analysis(clean_text)\n",
        "        readability_results = readability_analysis(clean_text)\n",
        "        word_results = word_analysis(clean_text)\n",
        "        pronoun_results = personal_pronouns(clean_text)\n",
        "        syllable_results = calculate_syllable_per_word(clean_text)\n",
        "\n",
        "        final_results = {**sentiment_results, **readability_results, **word_results, **pronoun_results, **syllable_results}\n",
        "\n",
        "        for key in final_results:\n",
        "            df.at[index, key.upper()] = final_results[key]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process {url_id}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Y2Aazmxgd6rV"
      },
      "outputs": [],
      "source": [
        "# Converting df to excel file\n",
        "\n",
        "df.to_excel(\"output.xlsx\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
